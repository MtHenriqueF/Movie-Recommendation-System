{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["KQUy6e8pPXYn","dgccqgRvTvlm","k47b7A7lTy_0","J4IUmJFpT5yE","bt9kf877T-ZJ","wqlYu_zxUARy","dt0G845uUJAr","KjwmcknbHgUT"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1) Setup: diret√≥rios, Drive, instala√ß√£o, Spark"],"metadata":{"id":"tWpa_jDDKFRx"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVxiW0gRJ-5B","executionInfo":{"status":"ok","timestamp":1764985551754,"user_tz":180,"elapsed":1247,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"c4f0f918-d168-42ad-d85b-fb0c723aade2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","BASE_DIR: /content/drive/MyDrive/Big Data/Trab Final BigData\n","DATA_DIR: /content/drive/MyDrive/Big Data/Trab Final BigData/dados_brutos\n","PROCESSED_DIR: /content/drive/MyDrive/Big Data/Trab Final BigData/dados_processados\n","MODEL_DIR: /content/drive/MyDrive/Big Data/Trab Final BigData/modelos\n","METRICS_DIR: /content/drive/MyDrive/Big Data/Trab Final BigData/metricas\n"]}],"source":["# ============================================================\n","# SE√á√ÉO 0 ‚Äì Setup de ambiente, Google Drive e diret√≥rios base\n","# ============================================================\n","import os\n","import json\n","from datetime import datetime\n","\n","# Monta o Google Drive se estiver no Colab\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    IN_COLAB = True\n","except ImportError:\n","    IN_COLAB = False\n","\n","# Caminhos base\n","BASE_DIR = \"/content/drive/MyDrive/Big Data/Trab Final BigData\"\n","# BASE_DIR = \"/content/drive/MyDrive/data-science/Big data - aula/Trab Final BigData\"\n","DATA_DIR = f\"{BASE_DIR}/dados_brutos\"\n","PROCESSED_DIR = f\"{BASE_DIR}/dados_processados\"\n","MODEL_DIR = f\"{BASE_DIR}/modelos\"\n","METRICS_DIR = f\"{BASE_DIR}/metricas\"\n","\n","# Criar pastas se n√£o existirem\n","for path in [DATA_DIR, PROCESSED_DIR, MODEL_DIR, METRICS_DIR]:\n","    os.makedirs(path, exist_ok=True)\n","\n","print(\"BASE_DIR:\", BASE_DIR)\n","print(\"DATA_DIR:\", DATA_DIR)\n","print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n","print(\"MODEL_DIR:\", MODEL_DIR)\n","print(\"METRICS_DIR:\", METRICS_DIR)\n"]},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 1 ‚Äì Instala√ß√£o de depend√™ncias (apenas em Colab)\n","# ============================================================\n","# Esta c√©lula √© Notebook-only. Em um script train.py voc√™ pode\n","# substituir por um requirements.txt ou instalar manualmente.\n","\n","if IN_COLAB:\n","    # Java para Spark\n","    !apt-get update > /dev/null\n","    !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","    !pip install pyspark matplotlib pandas seaborn --quiet\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVwxQYCZmxkT","executionInfo":{"status":"ok","timestamp":1764985569553,"user_tz":180,"elapsed":17796,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"b240e442-d044-403f-b338-9ac5beed02e1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 2 ‚Äì Cria√ß√£o da SparkSession principal\n","# ============================================================\n","from pyspark.sql import SparkSession\n","\n","spark = (\n","    SparkSession.builder\n","    .appName(\"MovieLens-ALS-EndToEnd\")\n","    .config(\"spark.driver.memory\", \"8g\")\n","    .config(\"spark.sql.shuffle.partitions\", \"200\")\n","    .getOrCreate()\n",")\n","\n","spark.sparkContext.setCheckpointDir(os.path.join(BASE_DIR, \"checkpoint_spark\"))\n","\n","print(\"SparkSession criada com sucesso!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prt8KJH8mznk","executionInfo":{"status":"ok","timestamp":1764985583465,"user_tz":180,"elapsed":13897,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"cc28ba4e-67be-454a-b282-2fa7882f2ded"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["SparkSession criada com sucesso!\n"]}]},{"cell_type":"markdown","source":["#2) Ingest√£o Inteligente com cache (CSV ‚Üí Parquet)"],"metadata":{"id":"KQUy6e8pPXYn"}},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 3 ‚Äì Download e ingest√£o inteligente do MovieLens\n","# ============================================================\n","import os\n","\n","# Caminhos para o zip e para a pasta descompactada\n","ML_ZIP_PATH = os.path.join(DATA_DIR, \"ml-latest.zip\")\n","ML_EXTRACT_DIR = os.path.join(DATA_DIR, \"ml-latest\")\n","\n","# 3.1 ‚Äì Baixar zip somente se ainda n√£o existir (wget -nc)\n","if IN_COLAB:\n","    # Aqui vou chamar via shell, mas mantendo a sem√¢ntica do -nc.\n","    if not os.path.exists(ML_ZIP_PATH):\n","        !wget -nc https://files.grouplens.org/datasets/movielens/ml-latest.zip -P \"$DATA_DIR\"\n","    else:\n","        print(\"‚úÖ Arquivo ZIP j√° existe, n√£o ser√° baixado novamente.\")\n","else:\n","    print(\"Rodando fora do Colab: certifique-se de que o arquivo ml-latest.zip j√° est√° em\", ML_ZIP_PATH)\n","\n","# 3.2 ‚Äì Descompactar apenas se a pasta ainda n√£o existir\n","if IN_COLAB:\n","    if not os.path.exists(ML_EXTRACT_DIR):\n","        !unzip -o \"$ML_ZIP_PATH\" -d \"$DATA_DIR\"\n","    else:\n","        print(\"‚úÖ Pasta j√° descompactada, pulando unzip.\")\n","else:\n","    print(\"Rodando localmente: certifique-se de que a pasta 'ml-latest' j√° foi descompactada dentro de\", DATA_DIR)\n","\n","print(\"Arquivos brutos esperados em:\", ML_EXTRACT_DIR)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m33r3pQYPFvl","executionInfo":{"status":"ok","timestamp":1764985583518,"user_tz":180,"elapsed":46,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"6aa74630-e512-42c2-b15b-554f43d1323a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Arquivo ZIP j√° existe, n√£o ser√° baixado novamente.\n","‚úÖ Pasta j√° descompactada, pulando unzip.\n","Arquivos brutos esperados em: /content/drive/MyDrive/Big Data/Trab Final BigData/dados_brutos/ml-latest\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 4 ‚Äì ETL com l√≥gica de cache (CSV -> Parquet)\n","#      - Se Parquet existir em PROCESSED_DIR, pular leitura do CSV\n","#      - Caso contr√°rio, ler CSV, tratar, salvar Parquet\n","# ============================================================\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import IntegerType, FloatType\n","\n","RATINGS_PARQUET_PATH = os.path.join(PROCESSED_DIR, \"ratings_parquet\")\n","MOVIES_PARQUET_PATH = os.path.join(PROCESSED_DIR, \"movies_parquet\")\n","\n","def load_or_create_processed_data():\n","    \"\"\"Carrega ratings e movies em formato Parquet, ou cria a partir dos CSVs se ainda n√£o existirem.\"\"\"\n","    if os.path.exists(RATINGS_PARQUET_PATH) and os.path.exists(MOVIES_PARQUET_PATH):\n","        print(\"‚úÖ Encontrado Parquet processado. Carregando de PROCESSED_DIR...\")\n","        ratings = spark.read.parquet(RATINGS_PARQUET_PATH)\n","        movies = spark.read.parquet(MOVIES_PARQUET_PATH)\n","        return ratings, movies\n","\n","    print(\"‚öôÔ∏è  Parquet n√£o encontrado. Lendo CSV bruto e processando...\")\n","\n","    ratings_csv_path = os.path.join(ML_EXTRACT_DIR, \"ratings.csv\")\n","    movies_csv_path = os.path.join(ML_EXTRACT_DIR, \"movies.csv\")\n","\n","    # Leitura bruta\n","    df_r_raw = spark.read.csv(ratings_csv_path, header=True, inferSchema=True)\n","    df_m_raw = spark.read.csv(movies_csv_path, header=True, inferSchema=True)\n","\n","    # Cast de tipos e sele√ß√£o de colunas relevantes\n","    ratings = (\n","        df_r_raw\n","        .select(\n","            F.col(\"userId\").cast(IntegerType()).alias(\"userId\"),\n","            F.col(\"movieId\").cast(IntegerType()).alias(\"movieId\"),\n","            F.col(\"rating\").cast(FloatType()).alias(\"rating\"),\n","            F.col(\"timestamp\").cast(\"long\").alias(\"timestamp\")\n","        )\n","    )\n","\n","    movies = (\n","        df_m_raw\n","        .select(\n","            F.col(\"movieId\").cast(IntegerType()).alias(\"movieId\"),\n","            F.col(\"title\").cast(\"string\").alias(\"title\"),\n","            F.col(\"genres\").cast(\"string\").alias(\"genres\")\n","        )\n","    )\n","\n","    # Tratamento de nulos\n","    ratings_before_nulls = ratings.count()\n","    ratings = ratings.dropna(subset=[\"userId\", \"movieId\", \"rating\"])\n","    ratings_after_nulls = ratings.count()\n","\n","    movies_before_nulls = movies.count()\n","    movies = movies.dropna(subset=[\"movieId\", \"title\", \"genres\"])\n","    movies_after_nulls = movies.count()\n","\n","    # Remo√ß√£o de ratings fora do range esperado [0.5, 5.0]\n","    ratings_before_range = ratings.count()\n","    ratings = ratings.filter((F.col(\"rating\") >= 0.5) & (F.col(\"rating\") <= 5.0))\n","    ratings_after_range = ratings.count()\n","\n","    # Tratamento de duplicatas em ratings: mesmo (userId, movieId)\n","    ratings_before_dups = ratings.count()\n","    ratings = (\n","        ratings\n","        .groupBy(\"userId\", \"movieId\")\n","        .agg(F.avg(\"rating\").alias(\"rating\"))\n","    )\n","    ratings_after_dups = ratings.count()\n","\n","    # Tratamento de duplicatas em movies: mesmo movieId\n","    movies_before_dups = movies.count()\n","    movies = movies.dropDuplicates([\"movieId\"])\n","    movies_after_dups = movies.count()\n","\n","    # Salvar Parquet\n","    ratings.write.mode(\"overwrite\").parquet(RATINGS_PARQUET_PATH)\n","    movies.write.mode(\"overwrite\").parquet(MOVIES_PARQUET_PATH)\n","\n","    print(\"‚úÖ Dados processados e salvos em formato Parquet.\")\n","\n","    # Retorna tamb√©m alguns metadados de ETL para registro\n","    etl_metrics = {\n","        \"ratings\": {\n","            \"before_nulls\": ratings_before_nulls,\n","            \"after_nulls\": ratings_after_nulls,\n","            \"before_range_filter\": ratings_before_range,\n","            \"after_range_filter\": ratings_after_range,\n","            \"before_duplicates\": ratings_before_dups,\n","            \"after_duplicates\": ratings_after_dups,\n","            \"duplicates_removed\": ratings_before_dups - ratings_after_dups,\n","        },\n","        \"movies\": {\n","            \"before_nulls\": movies_before_nulls,\n","            \"after_nulls\": movies_after_nulls,\n","            \"before_duplicates\": movies_before_dups,\n","            \"after_duplicates\": movies_after_dups,\n","            \"duplicates_removed\": movies_before_dups - movies_after_dups,\n","        },\n","    }\n","\n","    return ratings, movies, etl_metrics\n","\n","# Chamada principal de ETL\n","etl_result = load_or_create_processed_data()\n","if len(etl_result) == 2:\n","    ratings_df, movies_df = etl_result\n","    etl_metrics = None\n","else:\n","    ratings_df, movies_df, etl_metrics = etl_result\n","\n","ratings_df.cache()\n","movies_df.cache()\n","\n","print(\"Ratings count:\", ratings_df.count())\n","print(\"Movies count:\", movies_df.count())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPLRrAwWm8A5","executionInfo":{"status":"ok","timestamp":1764985646336,"user_tz":180,"elapsed":62815,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"7dc53d92-7b2d-4916-bd03-cb3689e3ab80"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Encontrado Parquet processado. Carregando de PROCESSED_DIR...\n","Ratings count: 33832162\n","Movies count: 86537\n"]}]},{"cell_type":"markdown","source":["#3) Gera√ß√£o e salvamento das m√©tricas (schema, nulos, duplicatas)"],"metadata":{"id":"dgccqgRvTvlm"}},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 5 ‚Äì C√°lculo e salvamento de m√©tricas de qualidade\n","#      - Schemas\n","#      - Contagem de nulos\n","#      - Contagem de duplicatas\n","# ============================================================\n","import pandas as pd\n","import os\n","\n","NULL_COUNTS_PATH = os.path.join(METRICS_DIR, \"null_counts.csv\")\n","DUP_SUMMARY_PATH = os.path.join(METRICS_DIR, \"duplicates_summary.json\")\n","\n","def save_quality_metrics(ratings, movies, extra_etl_metrics=None):\n","    # Se j√° tiver tudo salvo, s√≥ carrega e N√ÉO recalcula nada\n","    if os.path.exists(NULL_COUNTS_PATH) and os.path.exists(DUP_SUMMARY_PATH):\n","        print(\"‚úÖ M√©tricas de qualidade j√° existem. Carregando do disco e pulando rec√°lculo...\")\n","        null_counts_df = pd.read_csv(NULL_COUNTS_PATH)\n","        with open(DUP_SUMMARY_PATH, \"r\") as f:\n","            duplicates_summary = json.load(f)\n","        return null_counts_df, duplicates_summary\n","\n","    # ----- Schema -----\n","    ratings_schema_json = json.loads(ratings.schema.json())\n","    movies_schema_json = json.loads(movies.schema.json())\n","\n","    with open(os.path.join(METRICS_DIR, \"ratings_schema.json\"), \"w\") as f:\n","        json.dump(ratings_schema_json, f, indent=2)\n","\n","    with open(os.path.join(METRICS_DIR, \"movies_schema.json\"), \"w\") as f:\n","        json.dump(movies_schema_json, f, indent=2)\n","\n","    # ----- Nulos por coluna -----\n","    def null_counts_df_func(df, dataset_name):\n","        null_counts = df.select([\n","            F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns\n","        ]).toPandas()\n","        null_counts = null_counts.T.reset_index()\n","        null_counts.columns = [\"column\", \"null_count\"]\n","        null_counts[\"dataset\"] = dataset_name\n","        return null_counts\n","\n","    ratings_nulls = null_counts_df_func(ratings, \"ratings\")\n","    movies_nulls = null_counts_df_func(movies, \"movies\")\n","\n","    all_nulls = pd.concat([ratings_nulls, movies_nulls], ignore_index=True)\n","    all_nulls.to_csv(NULL_COUNTS_PATH, index=False)\n","\n","    # ----- Duplicatas (mesma l√≥gica da ETL) -----\n","    duplicates_summary = {}\n","\n","    dup_ratings = (\n","        ratings.groupBy(\"userId\", \"movieId\")\n","        .agg(F.count(\"*\").alias(\"qtd_votos\"))\n","        .filter(F.col(\"qtd_votos\") > 1)\n","    )\n","    duplicates_summary[\"ratings\"] = {\n","        \"pairs_with_more_than_one_rating\": dup_ratings.count()\n","    }\n","\n","    dup_movies = (\n","        movies.groupBy(\"movieId\")\n","        .agg(F.count(\"*\").alias(\"qtd_registros\"))\n","        .filter(F.col(\"qtd_registros\") > 1)\n","    )\n","    duplicates_summary[\"movies\"] = {\n","        \"duplicate_movieIds\": dup_movies.count()\n","    }\n","\n","    if extra_etl_metrics is not None:\n","        duplicates_summary[\"etl\"] = extra_etl_metrics\n","\n","    with open(DUP_SUMMARY_PATH, \"w\") as f:\n","        json.dump(duplicates_summary, f, indent=2)\n","\n","    print(\"‚úÖ M√©tricas de qualidade salvas em METRICS_DIR.\")\n","    return all_nulls, duplicates_summary\n","\n","# chamada\n","null_counts_df, duplicates_summary = save_quality_metrics(\n","    ratings_df, movies_df, extra_etl_metrics=etl_metrics\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6FHzXa7Pd_y","executionInfo":{"status":"ok","timestamp":1764985647031,"user_tz":180,"elapsed":688,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"1c34310f-6611-4da1-e756-9001053276c1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ M√©tricas de qualidade j√° existem. Carregando do disco e pulando rec√°lculo...\n"]}]},{"cell_type":"markdown","source":["#4) Visualiza√ß√µes (3 gr√°ficos salvos em PNG no METRICS_DIR)"],"metadata":{"id":"k47b7A7lTy_0"}},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 6 ‚Äì Visualiza√ß√µes (vers√£o nova com seaborn)\n","#      1) Filmes por G√™nero (Barra %)\n","#      2) Nota M√©dia por G√™nero\n","#      3) Distribui√ß√£o das Notas (Donut Chart)\n","# ============================================================\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pyspark.sql import functions as F\n","\n","df_m = movies_df\n","df_r = ratings_df\n","\n","print(\"\\nüìä --- GERANDO GR√ÅFICOS (backend) ---\")\n","\n","# ---------------------------------------------------------\n","# 2. FILMES POR G√äNERO\n","# ---------------------------------------------------------\n","\n","filmes_por_genero_path = os.path.join(METRICS_DIR, \"filmes_por_genero.png\")\n","filmes_por_genero_csv = os.path.join(METRICS_DIR, \"filmes_por_genero.csv\")\n","\n","df_genres = df_m.withColumn(\"genre\", F.explode(F.split(F.col(\"genres\"), \"\\\\|\")))\n","\n","# Remove o \"g√™nero\" que na verdade √© um t√≠tulo de filme incorreto\n","df_genres = df_genres.filter(F.col(\"genre\") != ' We\\'re Comin\\' To Get Ya!\"\" (2014)\"')\n","\n","if os.path.exists(filmes_por_genero_path) and os.path.exists(filmes_por_genero_csv):\n","    print(\"‚úÖ Filmes por g√™nero j√° existem em disco. Pulando gera√ß√£o...\")\n","else:\n","    genre_counts = (\n","        df_genres.groupBy(\"genre\")\n","        .count()\n","        .orderBy(F.col(\"count\").desc())\n","        .toPandas()\n","    )\n","\n","    total_films = genre_counts[\"count\"].sum()\n","    genre_counts[\"percent\"] = genre_counts[\"count\"] / total_films * 100.0\n","    genre_counts.to_csv(filmes_por_genero_csv, index=False)\n","\n","    plt.figure(figsize=(12, 8))\n","    ax = sns.barplot(x=\"count\", y=\"genre\", data=genre_counts, palette=\"viridis\")\n","    plt.xlim([0, genre_counts[\"count\"].max() * 1.15])\n","    plt.title(\"Filmes por G√™nero (%)\")\n","    plt.xlabel(\"Quantidade\")\n","\n","    # üî¥ R√≥tulo s√≥ com a %, sem valor absoluto\n","    for container in ax.containers:\n","        ax.bar_label(\n","            container,\n","            fmt=lambda x: f\"{x/total_films*100:.1f}%\",\n","            padding=3\n","        )\n","\n","    plt.tight_layout()\n","    plt.savefig(filmes_por_genero_path, dpi=120, bbox_inches=\"tight\")\n","    plt.close()\n","    print(\"‚úÖ Filmes por g√™nero salvo em:\", filmes_por_genero_path)\n","\n","print(\"‚úÖ Filmes por g√™nero salvo em:\", filmes_por_genero_path)\n","\n","# ---------------------------------------------------------\n","# 3. NOTA M√âDIA POR G√äNERO\n","# ---------------------------------------------------------\n","\n","nota_media_genero_path = os.path.join(METRICS_DIR, \"nota_media_por_genero.png\")\n","nota_media_genero_csv = os.path.join(METRICS_DIR, \"nota_media_por_genero.csv\")\n","\n","if os.path.exists(nota_media_genero_path) and os.path.exists(nota_media_genero_csv):\n","    print(\"‚úÖ Nota m√©dia por g√™nero j√° existe em disco. Pulando gera√ß√£o...\")\n","else:\n","    avg_ratings_per_movie = df_r.groupBy(\"movieId\").agg(\n","        F.avg(\"rating\").alias(\"avg_rating\")\n","    )\n","\n","    genre_ratings = df_genres.join(avg_ratings_per_movie, \"movieId\")\n","\n","    avg_genre = (\n","        genre_ratings.groupBy(\"genre\")\n","        .agg(F.avg(\"avg_rating\").alias(\"mean_rating\"))\n","        .orderBy(F.col(\"mean_rating\").desc())\n","        .toPandas()\n","    )\n","\n","    avg_genre.to_csv(nota_media_genero_csv, index=False)\n","\n","    plt.figure(figsize=(12, 6))\n","    sns.barplot(x=\"mean_rating\", y=\"genre\", data=avg_genre, palette=\"magma\")\n","    plt.title(\"Nota M√©dia por G√™nero\")\n","    plt.xlabel(\"Nota M√©dia (0‚Äì5)\")\n","    plt.xlim(2, 4.5)\n","    plt.ylabel(\"G√™nero\")\n","    plt.tight_layout()\n","    plt.savefig(nota_media_genero_path, dpi=120, bbox_inches=\"tight\")\n","    plt.close()\n","    print(\"‚úÖ Nota m√©dia por g√™nero salva em:\", nota_media_genero_path)\n","\n","plt.close()\n","print(\"‚úÖ Nota m√©dia por g√™nero salva em:\", nota_media_genero_path)\n","\n","# ---------------------------------------------------------\n","# 4. DISTRIBUI√á√ÉO DAS NOTAS (Donut Chart)\n","# ---------------------------------------------------------\n","\n","dist_notas_path = os.path.join(METRICS_DIR, \"distribuicao_notas.png\")\n","dist_notas_csv = os.path.join(METRICS_DIR, \"distribuicao_notas.csv\")\n","\n","if os.path.exists(dist_notas_path) and os.path.exists(dist_notas_csv):\n","    print(\"‚úÖ Distribui√ß√£o das notas j√° existe em disco. Pulando gera√ß√£o...\")\n","else:\n","    rating_counts = (\n","        df_r.groupBy(\"rating\")\n","        .count()\n","        .orderBy(\"rating\")\n","        .toPandas()\n","    )\n","\n","    total_count = rating_counts[\"count\"].sum()\n","    rating_counts[\"percent\"] = rating_counts[\"count\"] / total_count * 100.0\n","    rating_counts.to_csv(dist_notas_csv, index=False)\n","\n","    plt.figure(figsize=(9, 9))\n","    wedges, texts, autotexts = plt.pie(\n","        rating_counts[\"count\"],\n","        labels=rating_counts[\"rating\"],\n","        autopct=\"%1.1f%%\",\n","        startangle=90,\n","        colors=sns.color_palette(\"pastel\"),\n","        pctdistance=0.85\n","    )\n","\n","    centre_circle = plt.Circle((0, 0), 0.70, fc=\"white\")\n","    fig = plt.gcf()\n","    fig.gca().add_artist(centre_circle)\n","\n","    plt.title(\"Distribui√ß√£o das Notas (Share %)\")\n","    plt.tight_layout()\n","    plt.savefig(dist_notas_path, dpi=120, bbox_inches=\"tight\")\n","    plt.close()\n","    print(\"‚úÖ Distribui√ß√£o das notas salva em:\", dist_notas_path)\n","\n","plt.close()\n","print(\"‚úÖ Distribui√ß√£o das notas salva em:\", dist_notas_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbWTdGJNTyDX","executionInfo":{"status":"ok","timestamp":1764985648103,"user_tz":180,"elapsed":1069,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"d0f5c376-522c-4e26-8707-430eb1306603"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìä --- GERANDO GR√ÅFICOS (backend) ---\n","‚úÖ Filmes por g√™nero j√° existem em disco. Pulando gera√ß√£o...\n","‚úÖ Filmes por g√™nero salvo em: /content/drive/MyDrive/Big Data/Trab Final BigData/metricas/filmes_por_genero.png\n","‚úÖ Nota m√©dia por g√™nero j√° existe em disco. Pulando gera√ß√£o...\n","‚úÖ Nota m√©dia por g√™nero salva em: /content/drive/MyDrive/Big Data/Trab Final BigData/metricas/nota_media_por_genero.png\n","‚úÖ Distribui√ß√£o das notas j√° existe em disco. Pulando gera√ß√£o...\n","‚úÖ Distribui√ß√£o das notas salva em: /content/drive/MyDrive/Big Data/Trab Final BigData/metricas/distribuicao_notas.png\n"]}]},{"cell_type":"markdown","source":["#5) Modelagem ALS com Tuning + CrossValidation + Persist√™ncia"],"metadata":{"id":"J4IUmJFpT5yE"}},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 7 ‚Äì Treino do modelo ALS com ParamGrid + CrossValidator\n","#      - L√≥gica de cache do modelo: se existir em MODEL_DIR, carregar\n","#      - Se n√£o existir: treina, avalia e salva BestModel\n","# ============================================================\n","import os\n","from pyspark.ml.recommendation import ALS, ALSModel\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","ALS_MODEL_PATH = os.path.join(MODEL_DIR, \"modelo_als_completo\")\n","\n","def train_or_load_als_model(ratings):\n","    # 7.1 ‚Äì Cache do modelo: se j√° existir salvo, apenas carregamos\n","    if os.path.exists(ALS_MODEL_PATH):\n","        print(\"‚úÖ Modelo ALS encontrado em disco. Carregando de:\", ALS_MODEL_PATH)\n","        best_model = ALSModel.load(ALS_MODEL_PATH)\n","        return best_model, None\n","\n","    print(\"‚öôÔ∏è  Modelo ALS n√£o encontrado. Iniciando pipeline de treino com CrossValidation...\")\n","\n","    from pyspark.sql.functions import count\n","\n","    user_counts = ratings.groupBy(\"userId\").agg(count(\"*\").alias(\"user_cnt\"))\n","    movie_counts = ratings.groupBy(\"movieId\").agg(count(\"*\").alias(\"movie_cnt\"))\n","\n","    ratings_filtered = (\n","        ratings\n","        .join(user_counts, \"userId\")\n","        .join(movie_counts, \"movieId\")\n","        .filter(\"user_cnt >= 5 AND movie_cnt >= 5\")\n","        .select(\"userId\", \"movieId\", \"rating\")\n","    )\n","\n","    ratings_filtered = ratings_filtered.cache()\n","    print(\"Registros ap√≥s filtro de densidade:\", ratings_filtered.count())\n","\n","    # 7.2 ‚Äì Split train/test\n","    train, test = ratings_filtered.randomSplit([0.8, 0.2], seed=42)\n","    train = train.cache()\n","    test = test.cache()\n","    print(\"Train count:\", train.count(), \"| Test count:\", test.count())\n","\n","    # 7.3 ‚Äì Configura√ß√£o base do ALS\n","    als = ALS(\n","        userCol=\"userId\",\n","        itemCol=\"movieId\",\n","        ratingCol=\"rating\",\n","        implicitPrefs=False,\n","        coldStartStrategy=\"drop\",\n","        nonnegative=True,\n","        checkpointInterval=10,\n","        maxIter=10\n","    )\n","\n","    # 7.4 ‚Äì Grade de hiperpar√¢metros\n","    param_grid = (\n","        ParamGridBuilder()\n","        .addGrid(als.rank, [10, 20, 40])\n","        .addGrid(als.regParam, [0.01, 0.05, 0.1])\n","        .addGrid(als.maxIter, [10, 15])\n","        .build()\n","    )\n","\n","    # 7.5 ‚Äì Avaliador\n","    rmse_evaluator = RegressionEvaluator(\n","        metricName=\"rmse\",\n","        labelCol=\"rating\",\n","        predictionCol=\"prediction\"\n","    )\n","\n","    # 7.6 ‚Äì CrossValidator (k-fold)\n","    cv = CrossValidator(\n","        estimator=als,\n","        estimatorParamMaps=param_grid,\n","        evaluator=rmse_evaluator,\n","        numFolds=3,\n","        parallelism=4\n","    )\n","\n","    # 7.7 ‚Äì Treino\n","    cv_model = cv.fit(train)\n","    best_model = cv_model.bestModel\n","\n","    print(\"‚úÖ Treino conclu√≠do. Melhores hiperpar√¢metros:\")\n","    print(\"  rank     =\", best_model._java_obj.parent().getRank())\n","    print(\"  regParam =\", best_model._java_obj.parent().getRegParam())\n","    print(\"  maxIter  =\", best_model._java_obj.parent().getMaxIter())\n","\n","    # 7.8 ‚Äì Avalia√ß√£o em teste\n","    predictions = best_model.transform(test)\n","    rmse_test = rmse_evaluator.evaluate(predictions)\n","\n","    print(f\"RMSE no conjunto de teste: {rmse_test:.4f}\")\n","\n","    # 7.9 ‚Äì Salvar modelo e m√©tricas\n","    best_model.save(ALS_MODEL_PATH)\n","    print(\"‚úÖ BestModel salvo em:\", ALS_MODEL_PATH)\n","\n","    als_metrics = {\n","        \"timestamp\": datetime.utcnow().isoformat(),\n","        \"best_params\": {\n","            \"rank\": best_model._java_obj.parent().getRank(),\n","            \"regParam\": best_model._java_obj.parent().getRegParam(),\n","            \"maxIter\": best_model._java_obj.parent().getMaxIter(),\n","        },\n","        \"rmse_test\": rmse_test,\n","    }\n","\n","    with open(os.path.join(METRICS_DIR, \"als_metrics.json\"), \"w\") as f:\n","        json.dump(als_metrics, f, indent=2)\n","\n","    print(\"‚úÖ M√©tricas do modelo ALS salvas em:\", os.path.join(METRICS_DIR, \"als_metrics.json\"))\n","\n","    return best_model, als_metrics\n","\n","als_model, als_metrics = train_or_load_als_model(ratings_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oUwj_v1UT3pX","outputId":"50b9e6ef-da67-4163-a709-67ae4f83e8af","executionInfo":{"status":"ok","timestamp":1764985653115,"user_tz":180,"elapsed":5008,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Modelo ALS encontrado em disco. Carregando de: /content/drive/MyDrive/Big Data/Trab Final BigData/modelos/modelo_als_completo\n"]}]},{"cell_type":"markdown","source":["#6) Agrupamentos e Parquets para gr√°ficos (top filmes, g√™neros, usu√°rios)"],"metadata":{"id":"bt9kf877T-ZJ"}},{"cell_type":"code","source":["TOP_MOVIES_PARQUET = os.path.join(PROCESSED_DIR, \"top_movies.parquet\")\n","GENRE_STATS_PARQUET = os.path.join(PROCESSED_DIR, \"genre_stats.parquet\")\n","TOP10_MOST_WATCHED_PARQUET = os.path.join(PROCESSED_DIR, \"top10_most_watched.parquet\")\n","TOP10_BEST_RATED_PARQUET = os.path.join(PROCESSED_DIR, \"top10_best_rated.parquet\")\n","\n","# --- Top filmes ---\n","if os.path.exists(TOP_MOVIES_PARQUET):\n","    print(\"‚úÖ top_movies.parquet j√° existe. Carregando...\")\n","    top_movies_spark = spark.read.parquet(TOP_MOVIES_PARQUET)\n","else:\n","    print(\"‚öôÔ∏è Gerando top_movies.parquet...\")\n","    top_movies_spark = (\n","        ratings_df.groupBy(\"movieId\")\n","        .agg(\n","            F.count(\"*\").alias(\"num_ratings\"),\n","            F.avg(\"rating\").alias(\"avg_rating\")\n","        )\n","        .join(movies_df, on=\"movieId\", how=\"left\")\n","    )\n","    top_movies_spark.write.mode(\"overwrite\").parquet(TOP_MOVIES_PARQUET)\n","\n","# Tabelas derivadas para o front\n","min_ratings = 50  # limiar para \"melhor nota\"\n","\n","top10_most_watched = (\n","    top_movies_spark\n","    .orderBy(F.desc(\"num_ratings\"), F.desc(\"avg_rating\"))\n","    .limit(10)\n",")\n","top10_most_watched.write.mode(\"overwrite\").parquet(TOP10_MOST_WATCHED_PARQUET)\n","\n","top10_best_rated = (\n","    top_movies_spark\n","    .filter(F.col(\"num_ratings\") >= min_ratings)\n","    .orderBy(F.desc(\"avg_rating\"), F.desc(\"num_ratings\"))\n","    .limit(10)\n",")\n","top10_best_rated.write.mode(\"overwrite\").parquet(TOP10_BEST_RATED_PARQUET)\n","\n","print(\"‚úÖ Top 10 mais assistidos e Top 10 melhor nota salvos em dados_processados/\")\n","\n","# --- Stats por g√™nero ---\n","if os.path.exists(GENRE_STATS_PARQUET):\n","    print(\"‚úÖ genre_stats.parquet j√° existe. Carregando...\")\n","    genre_stats = spark.read.parquet(GENRE_STATS_PARQUET)\n","else:\n","    if \"genres\" in movies_df.columns:\n","        movies_exploded = (\n","            movies_df\n","            .withColumn(\"genre\", F.explode(F.split(F.col(\"genres\"), \"\\\\|\")))\n","            .filter(F.col(\"genre\") != \"(no genres listed)\")\n","        )\n","\n","        ratings_genres = ratings_df.join(movies_exploded, on=\"movieId\", how=\"left\")\n","\n","        genre_stats = (\n","            ratings_genres.filter(F.col(\"genre\").isNotNull())\n","            .groupBy(\"genre\")\n","            .agg(\n","                F.count(\"*\").alias(\"num_ratings\"),\n","                F.avg(\"rating\").alias(\"avg_rating\")\n","            )\n","        )\n","        genre_stats.write.mode(\"overwrite\").parquet(GENRE_STATS_PARQUET)\n","        print(\"‚úÖ genre_stats.parquet salvo.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtXxaCtgT8Cv","executionInfo":{"status":"ok","timestamp":1764985656246,"user_tz":180,"elapsed":3124,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"c0b117f3-65d0-4d07-8496-eb1b3a761c01"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ top_movies.parquet j√° existe. Carregando...\n","‚úÖ Top 10 mais assistidos e Top 10 melhor nota salvos em dados_processados/\n","‚úÖ genre_stats.parquet j√° existe. Carregando...\n"]}]},{"cell_type":"markdown","source":["#7) Split train/test para o ALS"],"metadata":{"id":"wqlYu_zxUARy"}},{"cell_type":"code","source":["# Bloco 7 ‚Äì Train/Test split\n","\n","train_ratings, test_ratings = ratings_df.randomSplit([0.8, 0.2], seed=42)\n","train_ratings = train_ratings.cache()\n","test_ratings = test_ratings.cache()\n","\n","print(\"Train:\", train_ratings.count(), \"Test:\", test_ratings.count())\n"],"metadata":{"id":"-wkep2aBUEYy","executionInfo":{"status":"ok","timestamp":1764985656254,"user_tz":180,"elapsed":4,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#8) Avalia√ß√£o do modelo ALS pesado (RMSE + MAPE)"],"metadata":{"id":"dt0G845uUJAr"}},{"cell_type":"code","source":["# Bloco 8 ‚Äì Avalia√ß√£o do modelo ALS pesado (RMSE + MAPE)\n","\n","from pyspark.ml.recommendation import ALSModel\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.sql.functions import abs as F_abs, col, avg as F_avg\n","import json\n","import os\n","\n","heavy_model_path = f\"{MODEL_DIR}/modelo_als_completo\"\n","metrics_path = f\"{METRICS_DIR}/als_metrics.json\"\n","print(\"Caminho do modelo pesado:\", heavy_model_path)\n","\n","# Carrega modelo pesado; se n√£o existir, chama seu treino com CrossValidation\n","if os.path.exists(heavy_model_path):\n","    print(\"üîµ Encontrado modelo pesado (offline). Carregando:\", heavy_model_path)\n","    model = ALSModel.load(heavy_model_path)\n","else:\n","    print(\"üü† Nenhum modelo encontrado. Treinando modelo ALS (CrossValidation)...\")\n","    model, _ = train_or_load_als_model(ratings_df)\n","\n","# Avalia√ß√£o no conjunto de teste\n","evaluator = RegressionEvaluator(\n","    metricName=\"rmse\",\n","    labelCol=\"rating\",\n","    predictionCol=\"prediction\"\n",")\n","\n","predictions = model.transform(test_ratings)\n","\n","rmse = evaluator.evaluate(predictions)\n","print(\"RMSE no conjunto de teste:\", rmse)\n","\n","# MAPE\n","predictions_mape = predictions.withColumn(\n","    \"ape\",\n","    F_abs((col(\"rating\") - col(\"prediction\")) / col(\"rating\"))\n",")\n","mape_test = predictions_mape.select(F_avg(\"ape\").alias(\"mape\")).first()[\"mape\"]\n","print(f\"MAPE no conjunto de teste: {mape_test:.4f}\")\n","\n","# Salva m√©tricas para o front\n","with open(metrics_path, \"w\") as f:\n","    json.dump(\n","        {\n","            \"rmse\": float(rmse),\n","            \"mape\": float(mape_test)\n","        },\n","        f,\n","        indent=2\n","    )\n","\n","print(\"‚úÖ M√©tricas salvas em:\", metrics_path)\n"],"metadata":{"id":"X15iAXOOUGes","executionInfo":{"status":"ok","timestamp":1764985656262,"user_tz":180,"elapsed":3,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["#9) Recomenda√ß√µes pr√©-calculadas (Front)"],"metadata":{"id":"KjwmcknbHgUT"}},{"cell_type":"code","source":["# Bloco 9 ‚Äì Recomenda√ß√µes pr√©-calculadas para ~600 usu√°rios\n","from pyspark.sql.functions import explode, col\n","\n","USER_RECS_PARQUET_NESTED = os.path.join(PROCESSED_DIR, \"user_recs_600.parquet\")\n","USER_RECS_PARQUET_FLAT = os.path.join(PROCESSED_DIR, \"user_recs_600_flat.parquet\")\n","\n","print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n","\n","if os.path.exists(USER_RECS_PARQUET_FLAT):\n","    print(\"‚úÖ user_recs_600_flat.parquet j√° existe em:\", USER_RECS_PARQUET_FLAT)\n","else:\n","    print(\"‚öôÔ∏è Gerando recomenda√ß√µes pr√©-calculadas para ~600 usu√°rios...\")\n","\n","    try:\n","        modelo_para_recs = model\n","    except NameError:\n","        modelo_para_recs = als_model\n","\n","    users_subset = (\n","        ratings_df\n","        .select(\"userId\")\n","        .distinct()\n","        .orderBy(\"userId\")\n","        .limit(600)\n","    )\n","\n","    # 1) gera recomenda√ß√µes aninhadas (array<struct<movieId, rating>>)\n","    recs_600 = modelo_para_recs.recommendForUserSubset(users_subset, 50)\n","\n","    # salva vers√£o aninhada\n","    recs_600.write.mode(\"overwrite\").parquet(USER_RECS_PARQUET_NESTED)\n","\n","    # 2) FLATTEN em Spark: (userId, movieId)\n","    recs_flat = (\n","        recs_600\n","        .select(\"userId\", explode(\"recommendations\").alias(\"rec\"))\n","        .select(\n","            col(\"userId\"),\n","            col(\"rec.movieId\").alias(\"movieId\"),\n","            col(\"rec.rating\").alias(\"predicted_rating\")\n","        )\n","    )\n","\n","    recs_flat.write.mode(\"overwrite\").parquet(USER_RECS_PARQUET_FLAT)\n","\n","    print(\"‚úÖ Vers√£o aninhada salva em:\", USER_RECS_PARQUET_NESTED)\n","    print(\"‚úÖ Vers√£o flatten salva em:\", USER_RECS_PARQUET_FLAT)\n"],"metadata":{"id":"uzZ6BF-8tiY7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764985656271,"user_tz":180,"elapsed":6,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"outputId":"7ba6c166-6074-4abc-c41a-c81ee0b21e95"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["PROCESSED_DIR: /content/drive/MyDrive/Big Data/Trab Final BigData/dados_processados\n","‚úÖ user_recs_600_flat.parquet j√° existe em: /content/drive/MyDrive/Big Data/Trab Final BigData/dados_processados/user_recs_600_flat.parquet\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# SE√á√ÉO 8 ‚Äì Encerramento\n","# ============================================================\n","spark.stop()\n","print(\"SparkSession encerrada.\")"],"metadata":{"id":"Tlki-pinnQaE","executionInfo":{"status":"ok","timestamp":1764985656651,"user_tz":180,"elapsed":377,"user":{"displayName":"Marcio Sirimarco","userId":"10904123349350111116"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b39cfda-8429-4bc4-d4ae-3f81cb939f25"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["SparkSession encerrada.\n"]}]}]}